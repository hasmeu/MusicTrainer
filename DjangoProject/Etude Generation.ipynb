{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-02T00:52:24.325758Z",
     "start_time": "2025-07-02T00:51:15.588258Z"
    }
   },
   "source": [
    "# Install a potentially more compatible version of transformers and datasets\n",
    "# Install a potentially more compatible version of transformers, datasets, and accelerate\n",
    "!pip install datasets==2.16.1 transformers==4.38.0 peft==0.8.2 accelerate==0.27.2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.16.1\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers==4.38.0\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "Collecting peft==0.8.2\n",
      "  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting accelerate==0.27.2\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (20.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.70.16)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.12.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from transformers==4.38.0) (2024.11.6)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0)\n",
      "  Downloading tokenizers-0.15.2.tar.gz (320 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [48 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\smith\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Downloading rustup-init from https://static.rust-lang.org/rustup/dist/x86_64-pc-windows-msvc/rustup-init.exe\n",
      "      \n",
      "      Downloading rustup-init:   0%|          | 0.00/13.6M [00:00<?, ?B/s]\n",
      "      Downloading rustup-init:   3%|Ã¢â€“Å½         | 459k/13.6M [00:00<00:02, 4.53MB/s]\n",
      "      Downloading rustup-init:   9%|Ã¢â€“Å          | 1.17M/13.6M [00:00<00:02, 5.98MB/s]\n",
      "      Downloading rustup-init:  14%|Ã¢â€“Ë†Ã¢â€“\\x8d        | 1.87M/13.6M [00:00<00:01, 6.42MB/s]\n",
      "      Downloading rustup-init:  19%|Ã¢â€“Ë†Ã¢â€“â€°        | 2.57M/13.6M [00:00<00:01, 6.61MB/s]\n",
      "      Downloading rustup-init:  24%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“\\x8d       | 3.28M/13.6M [00:00<00:01, 6.74MB/s]\n",
      "      Downloading rustup-init:  29%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“â€°       | 4.00M/13.6M [00:00<00:01, 6.85MB/s]\n",
      "      Downloading rustup-init:  35%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“\\x8d      | 4.71M/13.6M [00:00<00:01, 6.87MB/s]\n",
      "      Downloading rustup-init:  40%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†      | 5.43M/13.6M [00:00<00:01, 6.97MB/s]\n",
      "      Downloading rustup-init:  45%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’     | 6.14M/13.6M [00:00<00:01, 6.99MB/s]\n",
      "      Downloading rustup-init:  50%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†     | 6.84M/13.6M [00:01<00:00, 6.83MB/s]\n",
      "      Downloading rustup-init:  56%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’    | 7.53M/13.6M [00:01<00:00, 6.50MB/s]\n",
      "      Downloading rustup-init:  60%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†    | 8.18M/13.6M [00:01<00:00, 6.19MB/s]\n",
      "      Downloading rustup-init:  65%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’   | 8.81M/13.6M [00:01<00:00, 6.12MB/s]\n",
      "      Downloading rustup-init:  70%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“â€°   | 9.44M/13.6M [00:01<00:00, 5.95MB/s]\n",
      "      Downloading rustup-init:  74%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“\\x8d  | 10.1M/13.6M [00:01<00:00, 5.93MB/s]\n",
      "      Downloading rustup-init:  79%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å   | 10.7M/13.6M [00:01<00:00, 5.84MB/s]\n",
      "      Downloading rustup-init:  83%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å½ | 11.2M/13.6M [00:01<00:00, 5.81MB/s]\n",
      "      Downloading rustup-init:  87%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“â€¹ | 11.8M/13.6M [00:01<00:00, 5.77MB/s]\n",
      "      Downloading rustup-init:  92%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“\\x8f| 12.4M/13.6M [00:01<00:00, 5.72MB/s]\n",
      "      Downloading rustup-init:  96%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Å’| 13.0M/13.6M [00:02<00:00, 5.81MB/s]\n",
      "      Downloading rustup-init: 100%|Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†Ã¢â€“Ë†| 13.6M/13.6M [00:02<00:00, 6.17MB/s]\n",
      "      Installing rust to C:\\Users\\smith\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      warn: installing msvc toolchain without its prerequisites\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: latest update on 2025-06-26, rust version 1.88.0 (6b00bc388 2025-06-23)\n",
      "      info: downloading component 'cargo'\n",
      "      info: downloading component 'rust-std'\n",
      "      info: downloading component 'rustc'\n",
      "      info: installing component 'cargo'\n",
      "      info: installing component 'rust-std'\n",
      "      info: installing component 'rustc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ã— Encountered error while generating package metadata.\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset with predefined splits\n",
    "dataset = load_dataset(\"sander-wood/melodyhub\")\n",
    "\n",
    "# Assuming the dataset has 'train' and 'validation' splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# You can optionally split the validation set to create a test set\n",
    "# For example, split the validation set into new validation and test sets\n",
    "# This approach keeps the original train set intact.\n",
    "validation_test_split = validation_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "new_validation_dataset = validation_test_split['train']  # This will be the new validation set\n",
    "test_dataset = validation_test_split['test']      # This will be your test set\n",
    "\n",
    "print(\"Original Train Set:\", train_dataset)\n",
    "print(\"New Validation Set:\", new_validation_dataset)\n",
    "print(\"Test Set:\", test_dataset)"
   ],
   "id": "f2561809d5eb9bdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# You can choose a different pre-trained tokenizer if it suits your needs better\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Now you can use this tokenizer to process your datasets\n",
    "# For example, tokenizing the 'text' column (assuming your dataset has a 'text' column)\n",
    "def tokenize_function(examples):\n",
    "    # Assuming the ABC notation is in a column named 'input' based on the error and likely dataset structure\n",
    "    # Original comment said 'text', but the error traceback is using 'input'.\n",
    "    return tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to your datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_new_validation_dataset = new_validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized Datasets:\")\n",
    "print(\"Tokenized Train Set:\", tokenized_train_dataset)\n",
    "print(\"Tokenized New Validation Set:\", tokenized_new_validation_dataset)\n",
    "print(\"Tokenized Test Set:\", tokenized_test_dataset)"
   ],
   "id": "c45aaeab7869687"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer, training_args\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the pre-trained model for sequence classification\n",
    "# Adjust the number of labels based on your classification task\n",
    "# num_labels = 2 # Example: Binary classification. Change this based on your actual number of classes.\n",
    "# The actual number of labels should be determined from the unique values in the 'output' column.\n",
    "unique_labels = set(train_dataset['output'])\n",
    "num_labels = len(unique_labels)\n",
    "print(f\"Number of unique labels found: {num_labels}\")\n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the unique labels from the training set\n",
    "label_encoder.fit(list(unique_labels))\n",
    "\n",
    "# Function to rename the 'output' column to 'labels' and remove original columns\n",
    "def prepare_dataset(example):\n",
    "    example['labels'] = label_encoder.transform([example['output']])[0] # Encode the label\n",
    "    return example\n",
    "\n",
    "# Apply the preparation function to your datasets and remove original columns\n",
    "tokenized_train_dataset = tokenized_train_dataset.map(prepare_dataset, remove_columns=['output', 'input', 'dataset', 'task'])\n",
    "tokenized_new_validation_dataset = tokenized_new_validation_dataset.map(prepare_dataset, remove_columns=['output', 'input', 'dataset', 'task'])\n",
    "tokenized_test_dataset = tokenized_test_dataset.map(prepare_dataset, remove_columns=['output', 'input', 'dataset', 'task'])\n",
    "\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=tokenized_train_dataset, # training dataset\n",
    "    eval_dataset=tokenized_new_validation_dataset,  # evaluation dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# You can also evaluate the model after training\n",
    "results = trainer.evaluate(tokenized_test_dataset)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ],
   "id": "eed609b71ba8c6d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
