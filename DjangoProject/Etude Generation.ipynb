{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-03T00:51:41.835979Z",
     "start_time": "2025-07-03T00:48:26.015767Z"
    }
   },
   "source": [
    "# Install a potentially more compatible version of transformers and datasets\n",
    "# Install a potentially more compatible version of transformers, datasets, and accelerate\n",
    "!pip install datasets==2.16.1 transformers==4.38.0 peft==0.8.2 accelerate==0.27.2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.16.1\n",
      "  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting transformers==4.38.0\n",
      "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "Collecting peft==0.8.2\n",
      "  Using cached peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting accelerate==0.27.2\n",
      "  Using cached accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting filelock (from datasets==2.16.1)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets==2.16.1)\n",
      "  Using cached numpy-2.3.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.16.1)\n",
      "  Using cached pyarrow-20.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.16.1)\n",
      "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets==2.16.1)\n",
      "  Using cached pandas-2.3.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Collecting tqdm>=4.62.1 (from datasets==2.16.1)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==2.16.1)\n",
      "  Using cached xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Using cached multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.16.1)\n",
      "  Using cached aiohttp-3.12.13-cp313-cp313-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets==2.16.1)\n",
      "  Using cached huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.0)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0)\n",
      "  Using cached tokenizers-0.15.2.tar.gz (320 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.0)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from peft==0.8.2) (7.0.0)\n",
      "Collecting torch>=1.13.0 (from peft==0.8.2)\n",
      "  Downloading torch-2.7.1-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.14.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading multidict-6.6.3-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.16.1)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2025.6.15)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.13.0->peft==0.8.2)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.13.0->peft==0.8.2)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (80.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.13.0->peft==0.8.2)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.16.1) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (3.0.2)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.16.1)\n",
      "  Downloading multiprocess-0.70.17-py313-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.16.1)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.16.1)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smith\\desktop\\web app research\\musictrainer\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/8.5 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/8.5 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.5 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 4.7/8.5 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.3/8.5 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.9/8.5 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 6.2 MB/s eta 0:00:00\n",
      "Downloading peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Downloading aiohttp-3.12.13-cp313-cp313-win_amd64.whl (446 kB)\n",
      "Downloading multidict-6.6.3-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading numpy-2.3.1-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/12.7 MB 7.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.1/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.4/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading pyarrow-20.0.0-cp313-cp313-win_amd64.whl (25.7 MB)\n",
      "   ---------------------------------------- 0.0/25.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/25.7 MB 7.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.9/25.7 MB 7.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.2/25.7 MB 7.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.8/25.7 MB 7.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.1/25.7 MB 7.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/25.7 MB 7.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.7 MB 7.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.5/25.7 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.1/25.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.4/25.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/25.7 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.3/25.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.9/25.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.4/25.7 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.7 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.6/25.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.7/25.7 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading torch-2.7.1-cp313-cp313-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/216.1 MB 7.1 MB/s eta 0:00:31\n",
      "    --------------------------------------- 2.9/216.1 MB 7.0 MB/s eta 0:00:31\n",
      "    --------------------------------------- 4.2/216.1 MB 7.1 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 5.8/216.1 MB 7.1 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 7.1/216.1 MB 7.1 MB/s eta 0:00:30\n",
      "   - -------------------------------------- 7.9/216.1 MB 6.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 7.9/216.1 MB 6.8 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 8.9/216.1 MB 5.6 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 10.0/216.1 MB 5.5 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 11.5/216.1 MB 5.6 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 12.8/216.1 MB 5.8 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 14.4/216.1 MB 5.9 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 15.7/216.1 MB 6.0 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 17.3/216.1 MB 6.1 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 18.6/216.1 MB 6.1 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 20.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 21.2/216.1 MB 6.2 MB/s eta 0:00:32\n",
      "   ---- ----------------------------------- 21.8/216.1 MB 3.2 MB/s eta 0:01:01\n",
      "   ---- ----------------------------------- 23.1/216.1 MB 3.3 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 24.4/216.1 MB 3.4 MB/s eta 0:00:57\n",
      "   ---- ----------------------------------- 26.0/216.1 MB 3.5 MB/s eta 0:00:55\n",
      "   ----- ---------------------------------- 27.3/216.1 MB 3.6 MB/s eta 0:00:53\n",
      "   ----- ---------------------------------- 28.8/216.1 MB 3.7 MB/s eta 0:00:51\n",
      "   ----- ---------------------------------- 30.1/216.1 MB 3.8 MB/s eta 0:00:50\n",
      "   ----- ---------------------------------- 30.9/216.1 MB 3.8 MB/s eta 0:00:49\n",
      "   ------ --------------------------------- 33.0/216.1 MB 3.9 MB/s eta 0:00:47\n",
      "   ------ --------------------------------- 34.3/216.1 MB 4.0 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 34.6/216.1 MB 4.0 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 35.7/216.1 MB 4.0 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 36.2/216.1 MB 3.9 MB/s eta 0:00:46\n",
      "   ------ --------------------------------- 36.7/216.1 MB 3.9 MB/s eta 0:00:46\n",
      "   ------- -------------------------------- 38.3/216.1 MB 4.0 MB/s eta 0:00:45\n",
      "   ------- -------------------------------- 39.8/216.1 MB 4.0 MB/s eta 0:00:44\n",
      "   ------- -------------------------------- 41.2/216.1 MB 4.1 MB/s eta 0:00:43\n",
      "   ------- -------------------------------- 42.7/216.1 MB 4.2 MB/s eta 0:00:42\n",
      "   -------- ------------------------------- 44.0/216.1 MB 4.2 MB/s eta 0:00:41\n",
      "   -------- ------------------------------- 45.6/216.1 MB 4.3 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 46.9/216.1 MB 4.3 MB/s eta 0:00:40\n",
      "   -------- ------------------------------- 48.5/216.1 MB 4.4 MB/s eta 0:00:39\n",
      "   --------- ------------------------------ 49.8/216.1 MB 4.4 MB/s eta 0:00:38\n",
      "   --------- ------------------------------ 51.4/216.1 MB 4.5 MB/s eta 0:00:37\n",
      "   --------- ------------------------------ 52.7/216.1 MB 4.5 MB/s eta 0:00:37\n",
      "   --------- ------------------------------ 54.0/216.1 MB 4.6 MB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 55.3/216.1 MB 4.6 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 55.8/216.1 MB 4.6 MB/s eta 0:00:36\n",
      "   ---------- ----------------------------- 57.4/216.1 MB 4.6 MB/s eta 0:00:35\n",
      "   ---------- ----------------------------- 58.7/216.1 MB 4.6 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 60.3/216.1 MB 4.7 MB/s eta 0:00:34\n",
      "   ----------- ---------------------------- 61.6/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 62.9/216.1 MB 4.7 MB/s eta 0:00:33\n",
      "   ----------- ---------------------------- 64.5/216.1 MB 4.8 MB/s eta 0:00:32\n",
      "   ------------ --------------------------- 66.1/216.1 MB 4.8 MB/s eta 0:00:32\n",
      "   ------------ --------------------------- 67.4/216.1 MB 4.8 MB/s eta 0:00:31\n",
      "   ------------ --------------------------- 68.9/216.1 MB 4.9 MB/s eta 0:00:31\n",
      "   ------------- -------------------------- 70.3/216.1 MB 4.9 MB/s eta 0:00:30\n",
      "   ------------- -------------------------- 71.8/216.1 MB 4.9 MB/s eta 0:00:30\n",
      "   ------------- -------------------------- 73.1/216.1 MB 5.0 MB/s eta 0:00:29\n",
      "   ------------- -------------------------- 74.7/216.1 MB 5.0 MB/s eta 0:00:29\n",
      "   -------------- ------------------------- 76.0/216.1 MB 5.0 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 77.6/216.1 MB 5.1 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 78.9/216.1 MB 5.1 MB/s eta 0:00:28\n",
      "   -------------- ------------------------- 80.5/216.1 MB 5.1 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 81.8/216.1 MB 5.1 MB/s eta 0:00:27\n",
      "   --------------- ------------------------ 83.4/216.1 MB 5.2 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 84.7/216.1 MB 5.2 MB/s eta 0:00:26\n",
      "   --------------- ------------------------ 86.0/216.1 MB 5.2 MB/s eta 0:00:26\n",
      "   ---------------- ----------------------- 87.6/216.1 MB 5.2 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 88.9/216.1 MB 5.2 MB/s eta 0:00:25\n",
      "   ---------------- ----------------------- 90.4/216.1 MB 5.3 MB/s eta 0:00:24\n",
      "   ---------------- ----------------------- 91.8/216.1 MB 5.3 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 93.1/216.1 MB 5.3 MB/s eta 0:00:24\n",
      "   ----------------- ---------------------- 94.6/216.1 MB 5.3 MB/s eta 0:00:23\n",
      "   ----------------- ---------------------- 95.9/216.1 MB 5.3 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 97.5/216.1 MB 5.4 MB/s eta 0:00:23\n",
      "   ------------------ --------------------- 98.8/216.1 MB 5.4 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 100.4/216.1 MB 5.4 MB/s eta 0:00:22\n",
      "   ------------------ --------------------- 101.7/216.1 MB 5.4 MB/s eta 0:00:22\n",
      "   ------------------- -------------------- 103.0/216.1 MB 5.4 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 104.6/216.1 MB 5.4 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 106.2/216.1 MB 5.5 MB/s eta 0:00:21\n",
      "   ------------------- -------------------- 107.5/216.1 MB 5.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 108.8/216.1 MB 5.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 110.4/216.1 MB 5.5 MB/s eta 0:00:20\n",
      "   -------------------- ------------------- 111.7/216.1 MB 5.5 MB/s eta 0:00:19\n",
      "   -------------------- ------------------- 113.2/216.1 MB 5.5 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 114.6/216.1 MB 5.6 MB/s eta 0:00:19\n",
      "   --------------------- ------------------ 116.1/216.1 MB 5.6 MB/s eta 0:00:18\n",
      "   --------------------- ------------------ 117.4/216.1 MB 5.6 MB/s eta 0:00:18\n",
      "   --------------------- ------------------ 118.8/216.1 MB 5.6 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 120.3/216.1 MB 5.6 MB/s eta 0:00:18\n",
      "   ---------------------- ----------------- 121.6/216.1 MB 5.6 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 123.2/216.1 MB 5.6 MB/s eta 0:00:17\n",
      "   ---------------------- ----------------- 124.0/216.1 MB 5.6 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 125.0/216.1 MB 5.6 MB/s eta 0:00:17\n",
      "   ----------------------- ---------------- 126.4/216.1 MB 5.6 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 127.9/216.1 MB 5.6 MB/s eta 0:00:16\n",
      "   ----------------------- ---------------- 129.5/216.1 MB 5.7 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 130.8/216.1 MB 5.7 MB/s eta 0:00:16\n",
      "   ------------------------ --------------- 132.4/216.1 MB 5.7 MB/s eta 0:00:15\n",
      "   ------------------------ --------------- 133.7/216.1 MB 5.7 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 135.3/216.1 MB 5.7 MB/s eta 0:00:15\n",
      "   ------------------------- -------------- 136.6/216.1 MB 5.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 138.1/216.1 MB 5.7 MB/s eta 0:00:14\n",
      "   ------------------------- -------------- 139.5/216.1 MB 5.7 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 141.0/216.1 MB 5.8 MB/s eta 0:00:14\n",
      "   -------------------------- ------------- 142.3/216.1 MB 5.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 143.9/216.1 MB 5.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 144.4/216.1 MB 5.8 MB/s eta 0:00:13\n",
      "   -------------------------- ------------- 145.8/216.1 MB 5.8 MB/s eta 0:00:13\n",
      "   --------------------------- ------------ 147.1/216.1 MB 5.8 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 148.6/216.1 MB 5.8 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 150.2/216.1 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 151.3/216.1 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 151.8/216.1 MB 5.8 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 152.6/216.1 MB 5.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 152.8/216.1 MB 5.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 153.4/216.1 MB 5.7 MB/s eta 0:00:12\n",
      "   ---------------------------- ----------- 154.7/216.1 MB 5.7 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 156.0/216.1 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 157.3/216.1 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 158.3/216.1 MB 5.7 MB/s eta 0:00:11\n",
      "   ----------------------------- ---------- 159.6/216.1 MB 5.7 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 160.7/216.1 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 162.3/216.1 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 163.6/216.1 MB 5.7 MB/s eta 0:00:10\n",
      "   ------------------------------ --------- 165.2/216.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 166.5/216.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 168.0/216.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 169.3/216.1 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 170.9/216.1 MB 5.8 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 172.2/216.1 MB 5.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 173.5/216.1 MB 5.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 175.1/216.1 MB 5.8 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 176.4/216.1 MB 5.8 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 178.0/216.1 MB 5.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 179.3/216.1 MB 5.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 180.9/216.1 MB 5.8 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 182.2/216.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 183.8/216.1 MB 5.9 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 185.1/216.1 MB 5.9 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 186.1/216.1 MB 5.9 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 186.6/216.1 MB 5.8 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 187.2/216.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 188.2/216.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 189.5/216.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 191.1/216.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 192.4/216.1 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 194.0/216.1 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 195.6/216.1 MB 6.5 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 196.9/216.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 198.4/216.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 199.8/216.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 201.3/216.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 202.6/216.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 203.9/216.1 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 205.5/216.1 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 207.1/216.1 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 208.4/216.1 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 210.0/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  211.3/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  212.9/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  214.2/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  215.7/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  216.0/216.1 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 216.1/216.1 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.3/6.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 7.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.5/6.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 6.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 6.1 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.3/2.0 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 6.4 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 7.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.0 MB 7.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.2/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.5/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 6.7 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for tokenizers: filename=tokenizers-0.15.2-cp313-cp313-win_amd64.whl size=2202288 sha256=ba6b0690ec37e200339f17e81ad96c7ea42be915b974734b59fb065ff18d6ba3\n",
      "  Stored in directory: c:\\users\\smith\\appdata\\local\\pip\\cache\\wheels\\9a\\9e\\b2\\23c414d0f2efa93232572701bcfa4b1f5ec9554abc8d380426\n",
      "Successfully built tokenizers\n",
      "Installing collected packages: pytz, mpmath, xxhash, tzdata, tqdm, sympy, safetensors, regex, pyarrow-hotfix, pyarrow, propcache, numpy, networkx, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, torch, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, accelerate, transformers, peft, datasets\n",
      "\n",
      "   ----------------------------------------  0/31 [pytz]\n",
      "   ----------------------------------------  0/31 [pytz]\n",
      "   - --------------------------------------  1/31 [mpmath]\n",
      "   - --------------------------------------  1/31 [mpmath]\n",
      "   - --------------------------------------  1/31 [mpmath]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   --- ------------------------------------  3/31 [tzdata]\n",
      "   ----- ----------------------------------  4/31 [tqdm]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   ------ ---------------------------------  5/31 [sympy]\n",
      "   --------- ------------------------------  7/31 [regex]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   ----------- ----------------------------  9/31 [pyarrow]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   -------------- ------------------------- 11/31 [numpy]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   --------------- ------------------------ 12/31 [networkx]\n",
      "   ---------------- ----------------------- 13/31 [multidict]\n",
      "   ------------------ --------------------- 14/31 [fsspec]\n",
      "   --------------------- ------------------ 17/31 [dill]\n",
      "   ------------------------ --------------- 19/31 [yarl]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   ------------------------- -------------- 20/31 [torch]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   --------------------------- ------------ 21/31 [pandas]\n",
      "   ---------------------------- ----------- 22/31 [multiprocess]\n",
      "   ----------------------------- ---------- 23/31 [huggingface-hub]\n",
      "   ----------------------------- ---------- 23/31 [huggingface-hub]\n",
      "   ----------------------------- ---------- 23/31 [huggingface-hub]\n",
      "   -------------------------------- ------- 25/31 [tokenizers]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [aiohttp]\n",
      "   ---------------------------------- ----- 27/31 [accelerate]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------ --- 28/31 [transformers]\n",
      "   ------------------------------------- -- 29/31 [peft]\n",
      "   ------------------------------------- -- 29/31 [peft]\n",
      "   -------------------------------------- - 30/31 [datasets]\n",
      "   -------------------------------------- - 30/31 [datasets]\n",
      "   -------------------------------------- - 30/31 [datasets]\n",
      "   ---------------------------------------- 31/31 [datasets]\n",
      "\n",
      "Successfully installed accelerate-0.27.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 datasets-2.16.1 dill-0.3.7 filelock-3.18.0 frozenlist-1.7.0 fsspec-2023.10.0 huggingface-hub-0.33.2 mpmath-1.3.0 multidict-6.6.3 multiprocess-0.70.15 networkx-3.5 numpy-2.3.1 pandas-2.3.0 peft-0.8.2 propcache-0.3.2 pyarrow-20.0.0 pyarrow-hotfix-0.7 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.15.2 torch-2.7.1 tqdm-4.67.1 transformers-4.38.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T00:54:10.670596Z",
     "start_time": "2025-07-03T00:52:24.836913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset with predefined splits\n",
    "dataset = load_dataset(\"sander-wood/melodyhub\")\n",
    "\n",
    "# Assuming the dataset has 'train' and 'validation' splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# You can optionally split the validation set to create a test set\n",
    "# For example, split the validation set into new validation and test sets\n",
    "# This approach keeps the original train set intact.\n",
    "validation_test_split = validation_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "new_validation_dataset = validation_test_split['train']  # This will be the new validation set\n",
    "test_dataset = validation_test_split['test']      # This will be your test set\n",
    "\n",
    "print(\"Original Train Set:\", train_dataset)\n",
    "print(\"New Validation Set:\", new_validation_dataset)\n",
    "print(\"Test Set:\", test_dataset)"
   ],
   "id": "f2561809d5eb9bdb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 8.40kB [00:00, 9.79MB/s]\n",
      "Downloading data: 100%|| 649M/649M [01:37<00:00, 6.62MB/s] \n",
      "Downloading data: 100%|| 7.93M/7.93M [00:00<00:00, 18.4MB/s]\n",
      "Generating train split: 1055046 examples [00:01, 746632.43 examples/s]\n",
      "Generating validation split: 12701 examples [00:00, 662511.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 1055046\n",
      "})\n",
      "New Validation Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 6350\n",
      "})\n",
      "Test Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 6351\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T01:04:35.384075Z",
     "start_time": "2025-07-03T00:54:10.692904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# You can choose a different pre-trained tokenizer if it suits your needs better\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Now you can use this tokenizer to process your datasets\n",
    "# For example, tokenizing the 'text' column (assuming your dataset has a 'text' column)\n",
    "def tokenize_function(examples):\n",
    "    # Assuming the ABC notation is in a column named 'input' based on the error and likely dataset structure\n",
    "    # Original comment said 'text', but the error traceback is using 'input'.\n",
    "    return tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply the tokenization to your datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_new_validation_dataset = new_validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\nTokenized Datasets:\")\n",
    "print(\"Tokenized Train Set:\", tokenized_train_dataset)\n",
    "print(\"Tokenized New Validation Set:\", tokenized_new_validation_dataset)\n",
    "print(\"Tokenized Test Set:\", tokenized_test_dataset)"
   ],
   "id": "c45aaeab7869687",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|| 1055046/1055046 [10:12<00:00, 1721.52 examples/s]\n",
      "Map: 100%|| 6350/6350 [00:03<00:00, 1616.14 examples/s]\n",
      "Map: 100%|| 6351/6351 [00:03<00:00, 1677.08 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Datasets:\n",
      "Tokenized Train Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1055046\n",
      "})\n",
      "Tokenized New Validation Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6350\n",
      "})\n",
      "Tokenized Test Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6351\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-03T01:04:50.113196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import hashlib\n",
    "\n",
    "# Improved Memory-efficient label encoder with unknown label handling\n",
    "class RobustLabelEncoder:\n",
    "    def __init__(self, unknown_token=\"<UNK>\"):\n",
    "        self.label_to_int = {}\n",
    "        self.int_to_label = {}\n",
    "        self.next_int = 0\n",
    "        self.unknown_token = unknown_token\n",
    "        self.unknown_id = None\n",
    "\n",
    "    def fit(self, labels):\n",
    "        # Add unknown token first\n",
    "        self.label_to_int[self.unknown_token] = self.next_int\n",
    "        self.int_to_label[self.next_int] = self.unknown_token\n",
    "        self.unknown_id = self.next_int\n",
    "        self.next_int += 1\n",
    "\n",
    "        # Add all unique labels\n",
    "        unique_labels = set(labels)\n",
    "        for label in unique_labels:\n",
    "            if label not in self.label_to_int:\n",
    "                self.label_to_int[label] = self.next_int\n",
    "                self.int_to_label[self.next_int] = label\n",
    "                self.next_int += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, labels):\n",
    "        # Handle unknown labels gracefully\n",
    "        return [self.label_to_int.get(label, self.unknown_id) for label in labels]\n",
    "\n",
    "    def fit_transform(self, labels):\n",
    "        return self.fit(labels).transform(labels)\n",
    "\n",
    "    def inverse_transform(self, encoded_labels):\n",
    "        return [self.int_to_label.get(encoded, self.unknown_token) for encoded in encoded_labels]\n",
    "\n",
    "# Debug: Check dataset structure first\n",
    "print(\"Checking dataset structure...\")\n",
    "sample = tokenized_train_dataset[0]\n",
    "print(f\"Available columns: {list(sample.keys())}\")\n",
    "\n",
    "# Collect all unique labels from ALL datasets to ensure consistent vocabulary\n",
    "print(\"Collecting all unique labels from all datasets...\")\n",
    "all_train_labels = set(train_dataset['output'])\n",
    "all_validation_labels = set(new_validation_dataset['output'])\n",
    "all_test_labels = set(test_dataset['output'])\n",
    "\n",
    "# Combine all labels to create complete vocabulary\n",
    "all_unique_labels = all_train_labels.union(all_validation_labels).union(all_test_labels)\n",
    "num_labels = len(all_unique_labels) + 1  # +1 for unknown token\n",
    "\n",
    "print(f\"Total unique labels found across all datasets: {len(all_unique_labels)}\")\n",
    "print(f\"Number of labels for model (including UNK): {num_labels}\")\n",
    "\n",
    "# Initialize the robust label encoder\n",
    "label_encoder = RobustLabelEncoder()\n",
    "\n",
    "# Fit the encoder on ALL unique labels from all datasets\n",
    "print(\"Fitting label encoder on complete vocabulary...\")\n",
    "label_encoder.fit(list(all_unique_labels))\n",
    "print(\"Label encoder fitted successfully!\")\n",
    "\n",
    "# Load the model with correct number of labels\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "\n",
    "# Function to prepare dataset with robust label encoding and error handling\n",
    "def prepare_dataset(example):\n",
    "    # Check if 'output' column exists\n",
    "    if 'output' not in example:\n",
    "        raise KeyError(f\"'output' column not found. Available columns: {list(example.keys())}\")\n",
    "\n",
    "    # Safely encode the label\n",
    "    try:\n",
    "        example['labels'] = label_encoder.transform([example['output']])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding label '{example['output']}': {e}\")\n",
    "        example['labels'] = label_encoder.unknown_id  # Use unknown token ID as fallback\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply the preparation function to your datasets\n",
    "print(\"Preparing datasets...\")\n",
    "try:\n",
    "    tokenized_train_dataset = tokenized_train_dataset.map(prepare_dataset)\n",
    "    tokenized_new_validation_dataset = tokenized_new_validation_dataset.map(prepare_dataset)\n",
    "    tokenized_test_dataset = tokenized_test_dataset.map(prepare_dataset)\n",
    "    print(\"Datasets prepared successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing datasets: {e}\")\n",
    "    print(\"Attempting to inspect dataset structure...\")\n",
    "\n",
    "    # If there's still an error, let's check the original datasets\n",
    "    print(f\"Original train dataset columns: {train_dataset.column_names}\")\n",
    "    print(f\"Tokenized train dataset columns: {tokenized_train_dataset.column_names}\")\n",
    "\n",
    "    # Check if 'output' exists in original but not in tokenized\n",
    "    if 'output' in train_dataset.column_names and 'output' not in tokenized_train_dataset.column_names:\n",
    "        print(\"'output' column was removed during tokenization. Re-adding it...\")\n",
    "\n",
    "        # Check if 'output' column exists before adding it\n",
    "        if 'output' not in tokenized_train_dataset.column_names:\n",
    "            tokenized_train_dataset = tokenized_train_dataset.add_column('output', train_dataset['output'])\n",
    "\n",
    "        if 'output' not in tokenized_new_validation_dataset.column_names:\n",
    "            tokenized_new_validation_dataset = tokenized_new_validation_dataset.add_column('output', new_validation_dataset['output'])\n",
    "\n",
    "        if 'output' not in tokenized_test_dataset.column_names:\n",
    "            tokenized_test_dataset = tokenized_test_dataset.add_column('output', test_dataset['output'])\n",
    "\n",
    "        # Now try preparing datasets again\n",
    "        tokenized_train_dataset = tokenized_train_dataset.map(prepare_dataset)\n",
    "        tokenized_new_validation_dataset = tokenized_new_validation_dataset.map(prepare_dataset)\n",
    "        tokenized_test_dataset = tokenized_test_dataset.map(prepare_dataset)\n",
    "        print(\"Datasets prepared successfully after re-adding output column!\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create a Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=tokenized_train_dataset, # training dataset\n",
    "    eval_dataset=tokenized_new_validation_dataset,  # evaluation dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# You can also evaluate the model after training\n",
    "print(\"Evaluating model...\")\n",
    "results = trainer.evaluate(tokenized_test_dataset)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ],
   "id": "b7088ae5d1b8ddfa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 20:04:52.066000 9624 .venv\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset structure...\n",
      "Available columns: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask']\n",
      "Collecting all unique labels from all datasets...\n",
      "Total unique labels found across all datasets: 848103\n",
      "Number of labels for model (including UNK): 848104\n",
      "Fitting label encoder on complete vocabulary...\n",
      "Label encoder fitted successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  94%|| 990999/1055046 [01:28<00:05, 11142.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error preparing datasets: [Errno 28] No space left on device\n",
      "Attempting to inspect dataset structure...\n",
      "Original train dataset columns: ['dataset', 'task', 'input', 'output']\n",
      "Tokenized train dataset columns: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask']\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
