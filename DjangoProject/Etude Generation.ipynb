{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-22T21:43:51.211541Z",
     "start_time": "2025-07-22T21:43:45.220177Z"
    }
   },
   "source": [
    "\n",
    "!pip install datasets==2.16.1 transformers==4.38.0 peft==0.8.2 accelerate==0.27.2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.16.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: transformers==4.38.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: peft==0.8.2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: accelerate==0.27.2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (0.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.7)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (3.12.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from transformers==4.38.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from transformers==4.38.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from transformers==4.38.0) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from peft==0.8.2) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from peft==0.8.2) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2025.6.15)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.8.2) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.16.1) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from pandas->datasets==2.16.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smith\\onedrive\\desktop\\ai music app\\musictrainer\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T21:44:04.160089Z",
     "start_time": "2025-07-22T21:44:03.157568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset with predefined splits\n",
    "dataset = load_dataset(\"sander-wood/melodyhub\")\n",
    "\n",
    "# Assuming the dataset has 'train' and 'validation' splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# You can optionally split the validation set to create a test set\n",
    "# For example, split the validation set into new validation and test sets\n",
    "# This approach keeps the original train set intact.\n",
    "validation_test_split = validation_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "new_validation_dataset = validation_test_split['train']  # This will be the new validation set\n",
    "test_dataset = validation_test_split['test']      # This will be your test set\n",
    "\n",
    "print(\"Original Train Set:\", train_dataset)\n",
    "print(\"New Validation Set:\", new_validation_dataset)\n",
    "print(\"Test Set:\", test_dataset)"
   ],
   "id": "f2561809d5eb9bdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 1055046\n",
      "})\n",
      "New Validation Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 6350\n",
      "})\n",
      "Test Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input', 'output'],\n",
      "    num_rows: 6351\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T21:59:05.344859Z",
     "start_time": "2025-07-22T21:44:07.115136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Initialize GPT-2 tokenizer for causal language modeling\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add padding token (GPT-2 doesn't have one by default)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# For causal language modeling, we need to prepare the data differently\n",
    "# We'll concatenate input and output for training the model to continue sequences\n",
    "def prepare_text_for_causal_lm(examples):\n",
    "    # Combine input and output for causal language modeling\n",
    "    # This teaches the model to generate continuations\n",
    "    combined_texts = []\n",
    "    for inp, out in zip(examples[\"input\"], examples[\"output\"]):\n",
    "        # Add a separator token between input and output\n",
    "        combined_text = f\"{inp} {tokenizer.eos_token} {out}\"\n",
    "        combined_texts.append(combined_text)\n",
    "\n",
    "    return {\"text\": combined_texts}\n",
    "\n",
    "# Apply the text preparation to your datasets\n",
    "prepared_train_dataset = train_dataset.map(prepare_text_for_causal_lm, batched=True)\n",
    "prepared_validation_dataset = new_validation_dataset.map(prepare_text_for_causal_lm, batched=True)\n",
    "prepared_test_dataset = test_dataset.map(prepare_text_for_causal_lm, batched=True)\n",
    "\n",
    "# Tokenization function for causal language modeling\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the combined text for language modeling\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Adjust based on your sequence lengths\n",
    "        padding=False    # We'll handle padding with DataCollator\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train_dataset = prepared_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_validation_dataset = prepared_validation_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = prepared_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove text column as we only need input_ids and attention_mask for training\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\", \"input\", \"output\"])\n",
    "tokenized_validation_dataset = tokenized_validation_dataset.remove_columns([\"text\", \"input\", \"output\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\", \"input\", \"output\"])\n",
    "\n",
    "# Set up data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"\\nTokenized Datasets for Causal Language Modeling:\")\n",
    "print(\"Tokenized Train Set:\", tokenized_train_dataset)\n",
    "print(\"Tokenized Validation Set:\", tokenized_validation_dataset)\n",
    "print(\"Tokenized Test Set:\", tokenized_test_dataset)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ],
   "id": "c45aaeab7869687",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1055046/1055046 [00:05<00:00, 203692.93 examples/s]\n",
      "Map: 100%|██████████| 6350/6350 [00:00<00:00, 39360.38 examples/s]\n",
      "Map: 100%|██████████| 6351/6351 [00:00<00:00, 49163.16 examples/s]\n",
      "Map: 100%|██████████| 1055046/1055046 [14:40<00:00, 1198.84 examples/s]\n",
      "Map: 100%|██████████| 6350/6350 [00:05<00:00, 1223.00 examples/s]\n",
      "Map: 100%|██████████| 6351/6351 [00:05<00:00, 1146.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Datasets for Causal Language Modeling:\n",
      "Tokenized Train Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1055046\n",
      "})\n",
      "Tokenized Validation Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6350\n",
      "})\n",
      "Tokenized Test Set: Dataset({\n",
      "    features: ['dataset', 'task', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 6351\n",
      "})\n",
      "Vocabulary size: 50257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-22T22:01:12.391672Z",
     "start_time": "2025-07-22T21:59:10.997564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "#  Check dataset structure first\n",
    "print(\"Checking dataset structure...\")\n",
    "sample = tokenized_train_dataset[0]\n",
    "print(f\"Available columns: {list(sample.keys())}\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./gpt2_etude_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./gpt2_logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_validation_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting causal language model training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "results = trainer.evaluate(tokenized_test_dataset)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model.save_pretrained(\"./trained_etude_gpt2\")\n",
    "tokenizer.save_pretrained(\"./trained_etude_gpt2\")\n",
    "print(\"Model and tokenizer saved!\")\n"
   ],
   "id": "b0cfa824537c42fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GPT2LMHeadModel, GPT2Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model = \u001B[43mGPT2LMHeadModel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgpt2\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m#  Check dataset structure first\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mChecking dataset structure...\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[39m\n\u001B[32m   3148\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   3149\u001B[39m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[32m   3150\u001B[39m     cached_file_kwargs = {\n\u001B[32m   3151\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mcache_dir\u001B[39m\u001B[33m\"\u001B[39m: cache_dir,\n\u001B[32m   3152\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mforce_download\u001B[39m\u001B[33m\"\u001B[39m: force_download,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3162\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m_commit_hash\u001B[39m\u001B[33m\"\u001B[39m: commit_hash,\n\u001B[32m   3163\u001B[39m     }\n\u001B[32m-> \u001B[39m\u001B[32m3164\u001B[39m     resolved_archive_file = \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcached_file_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3166\u001B[39m     \u001B[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001B[39;00m\n\u001B[32m   3167\u001B[39m     \u001B[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001B[39;00m\n\u001B[32m   3168\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m resolved_archive_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001B[32m   3169\u001B[39m         \u001B[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:398\u001B[39m, in \u001B[36mcached_file\u001B[39m\u001B[34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[39m\n\u001B[32m    395\u001B[39m user_agent = http_user_agent(user_agent)\n\u001B[32m    396\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    397\u001B[39m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m     resolved_file = \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m=\u001B[49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    408\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    413\u001B[39m     resolved_file = _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001B[39m, in \u001B[36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    111\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[32m    112\u001B[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001B[34m__name__\u001B[39m, has_token=has_token, kwargs=kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001B[39m, in \u001B[36mhf_hub_download\u001B[39m\u001B[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[32m    989\u001B[39m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[32m    990\u001B[39m         local_dir=local_dir,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1005\u001B[39m         local_files_only=local_files_only,\n\u001B[32m   1006\u001B[39m     )\n\u001B[32m   1007\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1008\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1009\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[32m   1010\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1011\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[32m   1012\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1014\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1015\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[32m   1017\u001B[39m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1018\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1019\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1020\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1021\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1022\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[32m   1023\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1024\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1025\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1161\u001B[39m, in \u001B[36m_hf_hub_download_to_cache_dir\u001B[39m\u001B[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[39m\n\u001B[32m   1158\u001B[39m \u001B[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001B[39;00m\n\u001B[32m   1160\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m WeakFileLock(lock_path):\n\u001B[32m-> \u001B[39m\u001B[32m1161\u001B[39m     \u001B[43m_download_to_tmp_and_move\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1162\u001B[39m \u001B[43m        \u001B[49m\u001B[43mincomplete_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m.incomplete\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1163\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdestination_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblob_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1164\u001B[39m \u001B[43m        \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1165\u001B[39m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1166\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1167\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1168\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1169\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1170\u001B[39m \u001B[43m        \u001B[49m\u001B[43metag\u001B[49m\u001B[43m=\u001B[49m\u001B[43metag\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1171\u001B[39m \u001B[43m        \u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxet_file_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1172\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1173\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os.path.exists(pointer_path):\n\u001B[32m   1174\u001B[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1725\u001B[39m, in \u001B[36m_download_to_tmp_and_move\u001B[39m\u001B[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001B[39m\n\u001B[32m   1718\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m xet_file_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1719\u001B[39m             logger.warning(\n\u001B[32m   1720\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mXet Storage is enabled for this repo, but the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mhf_xet\u001B[39m\u001B[33m'\u001B[39m\u001B[33m package is not installed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1721\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mFalling back to regular HTTP download. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1722\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1723\u001B[39m             )\n\u001B[32m-> \u001B[39m\u001B[32m1725\u001B[39m         \u001B[43mhttp_get\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1726\u001B[39m \u001B[43m            \u001B[49m\u001B[43murl_to_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1727\u001B[39m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1728\u001B[39m \u001B[43m            \u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m=\u001B[49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1729\u001B[39m \u001B[43m            \u001B[49m\u001B[43mresume_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1730\u001B[39m \u001B[43m            \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1731\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexpected_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1732\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1734\u001B[39m logger.info(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDownload complete. Moving file to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdestination_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1735\u001B[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:494\u001B[39m, in \u001B[36mhttp_get\u001B[39m\u001B[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001B[39m\n\u001B[32m    492\u001B[39m new_resume_size = resume_size\n\u001B[32m    493\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m494\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m.\u001B[49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconstants\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDOWNLOAD_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# filter out keep-alive new chunks\u001B[39;49;00m\n\u001B[32m    496\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\requests\\models.py:820\u001B[39m, in \u001B[36mResponse.iter_content.<locals>.generate\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    818\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    819\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m820\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw.stream(chunk_size, decode_content=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    821\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    822\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\urllib3\\response.py:1091\u001B[39m, in \u001B[36mHTTPResponse.stream\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1089\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1090\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m._fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1091\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1093\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[32m   1094\u001B[39m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\urllib3\\response.py:980\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt, decode_content, cache_content)\u001B[39m\n\u001B[32m    977\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) >= amt:\n\u001B[32m    978\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._decoded_buffer.get(amt)\n\u001B[32m--> \u001B[39m\u001B[32m980\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m flush_decoder = amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt != \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[32m    984\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\urllib3\\response.py:904\u001B[39m, in \u001B[36mHTTPResponse._raw_read\u001B[39m\u001B[34m(self, amt, read1)\u001B[39m\n\u001B[32m    901\u001B[39m fp_closed = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m._fp, \u001B[33m\"\u001B[39m\u001B[33mclosed\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    903\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._error_catcher():\n\u001B[32m--> \u001B[39m\u001B[32m904\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    905\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt != \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[32m    906\u001B[39m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[32m    907\u001B[39m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    912\u001B[39m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[32m    913\u001B[39m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[32m    914\u001B[39m         \u001B[38;5;28mself\u001B[39m._fp.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive\\Desktop\\AI Music App\\MusicTrainer\\.venv\\Lib\\site-packages\\urllib3\\response.py:887\u001B[39m, in \u001B[36mHTTPResponse._fp_read\u001B[39m\u001B[34m(self, amt, read1)\u001B[39m\n\u001B[32m    884\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read1()\n\u001B[32m    885\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    886\u001B[39m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m887\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._fp.read()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:479\u001B[39m, in \u001B[36mHTTPResponse.read\u001B[39m\u001B[34m(self, amt)\u001B[39m\n\u001B[32m    476\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt > \u001B[38;5;28mself\u001B[39m.length:\n\u001B[32m    477\u001B[39m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[32m    478\u001B[39m     amt = \u001B[38;5;28mself\u001B[39m.length\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m s = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[32m    481\u001B[39m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[32m    482\u001B[39m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[32m    483\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_conn()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    717\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot read from timed out object\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m719\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    720\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    721\u001B[39m     \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1300\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1301\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1302\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1303\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1305\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1306\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1139\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1140\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-03T01:04:50.113196Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0702 20:04:52.066000 9624 .venv\\Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset structure...\n",
      "Available columns: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask']\n",
      "Collecting all unique labels from all datasets...\n",
      "Total unique labels found across all datasets: 848103\n",
      "Number of labels for model (including UNK): 848104\n",
      "Fitting label encoder on complete vocabulary...\n",
      "Label encoder fitted successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  94%|█████████▍| 990999/1055046 [01:28<00:05, 11142.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error preparing datasets: [Errno 28] No space left on device\n",
      "Attempting to inspect dataset structure...\n",
      "Original train dataset columns: ['dataset', 'task', 'input', 'output']\n",
      "Tokenized train dataset columns: ['dataset', 'task', 'input', 'output', 'input_ids', 'attention_mask']\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smith\\Desktop\\Web App Research\\MusicTrainer\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "from transformers import RobertaForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import hashlib\n",
    "\n",
    "# Improved Memory-efficient label encoder with unknown label handling\n",
    "class RobustLabelEncoder:\n",
    "    def __init__(self, unknown_token=\"<UNK>\"):\n",
    "        self.label_to_int = {}\n",
    "        self.int_to_label = {}\n",
    "        self.next_int = 0\n",
    "        self.unknown_token = unknown_token\n",
    "        self.unknown_id = None\n",
    "\n",
    "    def fit(self, labels):\n",
    "        # Add unknown token first\n",
    "        self.label_to_int[self.unknown_token] = self.next_int\n",
    "        self.int_to_label[self.next_int] = self.unknown_token\n",
    "        self.unknown_id = self.next_int\n",
    "        self.next_int += 1\n",
    "\n",
    "        # Add all unique labels\n",
    "        unique_labels = set(labels)\n",
    "        for label in unique_labels:\n",
    "            if label not in self.label_to_int:\n",
    "                self.label_to_int[label] = self.next_int\n",
    "                self.int_to_label[self.next_int] = label\n",
    "                self.next_int += 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, labels):\n",
    "        # Handle unknown labels gracefully\n",
    "        return [self.label_to_int.get(label, self.unknown_id) for label in labels]\n",
    "\n",
    "    def fit_transform(self, labels):\n",
    "        return self.fit(labels).transform(labels)\n",
    "\n",
    "    def inverse_transform(self, encoded_labels):\n",
    "        return [self.int_to_label.get(encoded, self.unknown_token) for encoded in encoded_labels]\n",
    "\n",
    "# Debug: Check dataset structure first\n",
    "print(\"Checking dataset structure...\")\n",
    "sample = tokenized_train_dataset[0]\n",
    "print(f\"Available columns: {list(sample.keys())}\")\n",
    "\n",
    "# Collect all unique labels from ALL datasets to ensure consistent vocabulary\n",
    "print(\"Collecting all unique labels from all datasets...\")\n",
    "all_train_labels = set(train_dataset['output'])\n",
    "all_validation_labels = set(new_validation_dataset['output'])\n",
    "all_test_labels = set(test_dataset['output'])\n",
    "\n",
    "# Combine all labels to create complete vocabulary\n",
    "all_unique_labels = all_train_labels.union(all_validation_labels).union(all_test_labels)\n",
    "num_labels = len(all_unique_labels) + 1  # +1 for unknown token\n",
    "\n",
    "print(f\"Total unique labels found across all datasets: {len(all_unique_labels)}\")\n",
    "print(f\"Number of labels for model (including UNK): {num_labels}\")\n",
    "\n",
    "# Initialize the robust label encoder\n",
    "label_encoder = RobustLabelEncoder()\n",
    "\n",
    "# Fit the encoder on ALL unique labels from all datasets\n",
    "print(\"Fitting label encoder on complete vocabulary...\")\n",
    "label_encoder.fit(list(all_unique_labels))\n",
    "print(\"Label encoder fitted successfully!\")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_labels)\n",
    "\n",
    "# Function to prepare dataset with robust label encoding and error handling\n",
    "def prepare_dataset(example):\n",
    "    # Check if 'output' column exists\n",
    "    if 'output' not in example:\n",
    "        raise KeyError(f\"'output' column not found. Available columns: {list(example.keys())}\")\n",
    "\n",
    "    # Safely encode the label\n",
    "    try:\n",
    "        example['labels'] = label_encoder.transform([example['output']])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding label '{example['output']}': {e}\")\n",
    "        example['labels'] = label_encoder.unknown_id  # Use unknown token ID as fallback\n",
    "\n",
    "    return example\n",
    "\n",
    "# Apply the preparation function to your datasets\n",
    "print(\"Preparing datasets...\")\n",
    "try:\n",
    "    tokenized_train_dataset = tokenized_train_dataset.map(prepare_dataset)\n",
    "    tokenized_new_validation_dataset = tokenized_new_validation_dataset.map(prepare_dataset)\n",
    "    tokenized_test_dataset = tokenized_test_dataset.map(prepare_dataset)\n",
    "    print(\"Datasets prepared successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing datasets: {e}\")\n",
    "    print(\"Attempting to inspect dataset structure...\")\n",
    "\n",
    "    # If there's still an error, let's check the original datasets\n",
    "    print(f\"Original train dataset columns: {train_dataset.column_names}\")\n",
    "    print(f\"Tokenized train dataset columns: {tokenized_train_dataset.column_names}\")\n",
    "\n",
    "    # Check if 'output' exists in original but not in tokenized\n",
    "    if 'output' in train_dataset.column_names and 'output' not in tokenized_train_dataset.column_names:\n",
    "        print(\"'output' column was removed during tokenization. Re-adding it...\")\n",
    "\n",
    "        # Check if 'output' column exists before adding it\n",
    "        if 'output' not in tokenized_train_dataset.column_names:\n",
    "            tokenized_train_dataset = tokenized_train_dataset.add_column('output', train_dataset['output'])\n",
    "\n",
    "        if 'output' not in tokenized_new_validation_dataset.column_names:\n",
    "            tokenized_new_validation_dataset = tokenized_new_validation_dataset.add_column('output', new_validation_dataset['output'])\n",
    "\n",
    "        if 'output' not in tokenized_test_dataset.column_names:\n",
    "            tokenized_test_dataset = tokenized_test_dataset.add_column('output', test_dataset['output'])\n",
    "\n",
    "        tokenized_train_dataset = tokenized_train_dataset.map(prepare_dataset)\n",
    "        tokenized_new_validation_dataset = tokenized_new_validation_dataset.map(prepare_dataset)\n",
    "        tokenized_test_dataset = tokenized_test_dataset.map(prepare_dataset)\n",
    "        print(\"Datasets prepared successfully after re-adding output column!\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_new_validation_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Evaluating model...\")\n",
    "results = trainer.evaluate(tokenized_test_dataset)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ],
   "id": "b7088ae5d1b8ddfa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
